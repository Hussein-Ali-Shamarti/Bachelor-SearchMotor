import os
import sys
import io
import gzip
import traceback
import numpy as np
import openai
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from database import SessionLocal
from models import Article
from faiss_helper import rebuild_faiss_index

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Ensure correct encoding for logs
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

# Load environment variables
load_dotenv()

# Ensure OPENAI_API_KEY is correctly retrieved
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OpenAI API key is missing! Check your .env file.")

# Set the OpenAI API key
openai.api_key = OPENAI_API_KEY

# Initialize Flask app
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*", "methods": ["GET", "POST", "OPTIONS"]}})

# Initialize Sentence Transformer model
MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Load or rebuild FAISS index on startup
INDEX, IDS = rebuild_faiss_index()
if INDEX is None:
    print("FAISS index is empty! No embeddings found in the database.")

# Utility Functions
def split_text_into_chunks(text, max_tokens=3000):
    "Split text into smaller chunks to fit OpenAI's token limit."
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(current_chunk) * 2 > max_tokens:  # Rough estimate for tokens
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

def summarize_with_openai(text):
    "Summarize a large text using OpenAI's API."
    try:
        print("Starting summarization process...")
        chunks = split_text_into_chunks(text)
        summary_parts = []

        for chunk in chunks:
            try:
                # Shorten max_tokens even more for minimal token usage
                response = openai.ChatCompletion.create(
                    model="gpt-4",  # Using the chat model
                    messages=[
                        {"role": "system", "content": "You are a concise assistant."},
                        {"role": "user", "content": 
                            "Provide a very brief summary of the following text, highlighting only key points and maintaining brevity:\n\n" + chunk}
                    ],
                    max_tokens=50,  # Further reduced for a more concise summary
                    temperature=0.3  # Keep it focused and concise
                )

                print(f"OpenAI Response for chunk (length {len(chunk)}): {response}")

                # Check if the response is valid
                if 'choices' not in response or len(response.choices) == 0:
                    print(f"OpenAI response is missing choices: {response}")
                    raise ValueError("No summary generated by OpenAI.")

                summary_parts.append(response.choices[0].message['content'].strip())

            except openai.error.RateLimitError as e:
                print(f"Rate limit error: {e}")
                return "Rate limit exceeded. Please try again later."

            except openai.error.AuthenticationError as e:
                print(f"Authentication error: {e}")
                return "Authentication failed. Please check your API key."

            except openai.error.InvalidRequestError as e:
                print(f"Invalid request error: {e}")
                return "Invalid request. Please check the input text and parameters."

            except openai.error.OpenAIError as e:
                print(f"OpenAI API error: {str(e)}")
                return "Failed to generate summary due to an API error."

            except Exception as e:
                print(f"Error processing chunk: {e}")
                return "Failed to generate summary due to an error."

        final_summary = " ".join(summary_parts)
        print(f"Summary generated successfully. Length: {len(final_summary)} characters.")
        return final_summary

    except Exception as e:
        print(f"Unexpected error in summarize_with_openai: {str(e)}")
        return "Failed to generate summary."


# Routes
@app.route("/generate-embedding", methods=["POST"])
def generate_embedding():
    "Generate text embedding using the SentenceTransformer model."
    data = request.get_json()
    text = data.get("text", "").strip()
    if not text:
        return jsonify({"error": "Text cannot be empty"}), 400
    try:
        embedding = MODEL.encode(text).tolist()
        return jsonify({"embedding": embedding})
    except Exception as e:
        print(f"Error generating embedding: {str(e)}")
        return jsonify({"error": "Failed to generate embedding", "details": str(e)}), 500


@app.route("/ai-search", methods=["POST"])
def ai_search():
    "Search articles based on the provided embedding."
    try:
        data = request.get_json()
        print("Received search query:", data)

        if "embedding" not in data:
            return jsonify({"error": "Missing 'embedding' key"}), 400

        query_embedding = np.array(data["embedding"], dtype='float32').reshape(1, -1)

        if query_embedding.shape[1] != 384:
            return jsonify({"error": f"Invalid embedding dimension: {query_embedding.shape[1]}"}), 400

        k = data.get("k", 5)
        if INDEX is None:
            return jsonify({"error": "No articles found in FAISS index"}), 404

        distances, indices = INDEX.search(query_embedding, k)

        with SessionLocal() as session:
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(IDS):
                    article = session.query(Article).filter_by(id=IDS[idx]).first()
                    if article is None:
                        continue

                    results.append({
                        "id": article.id,
                        "title": article.title,
                        "abstract": article.abstract,
                        "author": article.author,
                        "publication_date": article.publication_date,
                        "pdf_url": article.pdf_url,
                        "keywords": article.keywords,
                        "isbn": article.isbn,
                        "distance": float(distances[0][i])
                    })

        if not results:
            return jsonify({"error": "No articles found for search results"}), 404

        return jsonify(results)

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": "Internal server error", "details": str(e)}), 500


@app.route("/article-text/<int:article_id>", methods=["GET"])
def get_article_text(article_id):
    "Retrieve the full text of an article by its ID."
    try:
        with SessionLocal() as session:
            article = session.query(Article).filter_by(id=article_id).first()
            if article is None:
                return jsonify({"error": f"Article with ID {article_id} not found"}), 404
            if not article.pdf_texts:
                return jsonify({"error": f"No PDF text available for article {article_id}"}), 404

            decompressed_text = gzip.decompress(article.pdf_texts).decode("utf-8")
            return jsonify({"text": decompressed_text})

    except Exception as e:
        traceback.print_exc()
        return jsonify({"error": "Internal server error"}), 500


@app.route("/article-summary/<int:article_id>", methods=["GET"])
def get_article_summary(article_id):
    "Generate and return a summary of an article's text."
    try:
        print(f"Fetching summary for article ID: {article_id}")

        # Fetch the article from the database
        with SessionLocal() as session:
            article = session.query(Article).filter_by(id=article_id).first()
            if article is None:
                print(f"Article {article_id} not found")
                return jsonify({"error": "Article not found"}), 404

            if not article.pdf_texts:
                print(f"No PDF text available for article {article_id}")
                return jsonify({"error": "No PDF text available"}), 404

            full_text = gzip.decompress(article.pdf_texts).decode("utf-8")
            print(f"Fetched article text for ID {article_id}, length: {len(full_text)} characters")

            # Check if the text is empty before summarizing
            if not full_text.strip():
                print(f"Article {article_id} text is empty")
                return jsonify({"error": "Article text is empty"}), 400

            # Generate the summary using OpenAI
            summary = summarize_with_openai(full_text)

            if not summary or summary.strip() == "":
                print(f"Empty summary returned for article {article_id}")
                return jsonify({"error": "Failed to generate summary"}), 500

            print(f"Generated summary for article {article_id}, length: {len(summary)} characters")
            return jsonify({"summary": summary})

    except openai.OpenAIError as e:
        print(f"OpenAI API error: {e}")
        return jsonify({"error": "Failed to generate summary due to an API error", "details": str(e)}), 500
    except Exception as e:
        print("Error generating article summary:", str(e))
        traceback.print_exc()
        return jsonify({"error": "Internal server error", "details": str(e)}), 500


# Run Flask
if __name__ == "__main__":
    print("Running Flask server on port 5001...")
    app.run(debug=True, host="0.0.0.0", port=5001)
